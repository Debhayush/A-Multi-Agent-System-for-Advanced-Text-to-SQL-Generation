{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:35:20.313699Z",
     "iopub.status.busy": "2025-09-17T12:35:20.313463Z",
     "iopub.status.idle": "2025-09-17T12:37:45.905553Z",
     "shell.execute_reply": "2025-09-17T12:37:45.904822Z",
     "shell.execute_reply.started": "2025-09-17T12:35:20.313673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.38.0 in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Collecting transformers>=4.38.0\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: peft>=0.9.0 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Collecting peft>=0.9.0\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: accelerate>=0.28.0 in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate>=0.28.0\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes>=0.41.3\n",
      "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
      "Collecting trl>=0.8.0\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.38.0)\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.38.0)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.9.0) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.9.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl>=0.8.0) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.8.0) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.38.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.38.0) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers>=4.38.0) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.0) (2025.6.15)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft>=0.9.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.9.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.9.0) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (3.12.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.9.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers>=4.38.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers>=4.38.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers>=4.38.0) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl>=0.8.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.8.0) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers>=4.38.0) (2024.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl>=0.8.0) (1.17.0)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, trl, peft, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.15.2\n",
      "    Uninstalling peft-0.15.2:\n",
      "      Successfully uninstalled peft-0.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.10.1 bitsandbytes-0.47.0 fsspec-2025.3.0 huggingface-hub-0.35.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.17.1 tokenizers-0.22.0 transformers-4.56.1 trl-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.38.0\" \"peft>=0.9.0\" \"accelerate>=0.28.0\" \"bitsandbytes>=0.41.3\" \"trl>=0.8.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "    print(\" Logging in with token from Kaggle secret...\")\n",
    "    login(token=hf_token)\n",
    "    print(\"Login successful.\")\n",
    "else:\n",
    "    print(\" Kaggle secret 'HF_TOKEN' not found. Please add it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T18:31:21.417436Z",
     "iopub.status.busy": "2025-09-15T18:31:21.417160Z",
     "iopub.status.idle": "2025-09-15T18:31:23.701794Z",
     "shell.execute_reply": "2025-09-15T18:31:23.701177Z",
     "shell.execute_reply.started": "2025-09-15T18:31:21.417414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Soft2012/sql_fine_tune_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "LLAMA2_CHAT_TEMPLATE = (\n",
    "    \"<s>[INST] {% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}\"\n",
    "    \"{{ message['content'] }} \"\n",
    "    \"{% else %}\"\n",
    "    \"[/INST] {{ message['content'] }} </s><s>[INST] \"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")\n",
    "tokenizer.chat_template = LLAMA2_CHAT_TEMPLATE\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T12:37:45.907581Z",
     "iopub.status.busy": "2025-09-17T12:37:45.907299Z",
     "iopub.status.idle": "2025-09-17T12:37:54.067832Z",
     "shell.execute_reply": "2025-09-17T12:37:54.067201Z",
     "shell.execute_reply.started": "2025-09-17T12:37:45.907555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import torch\n",
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "# from trl import SFTConfig, SFTTrainer\n",
    "# from huggingface_hub import login as hf_login\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"NousResearch/Llama-2-7b-chat-hf\"  \n",
    "OUTPUT_DIR = \"/kaggle/working/llama2-sft-qlora\"\n",
    "\n",
    "DATASET = \"Soft2012/sql_fine_tune_dataset\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "DATASET_TEXT_FIELD = \"text\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 1\n",
    "GRAD_ACC_STEPS = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "\n",
    "USE_BF16 = False   \n",
    "\n",
    "SMOKE_TEST = False\n",
    "SMOKE_TEST_SIZE = 1024\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('Config set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    ")\n",
    "\n",
    "\n",
    "def try_load_model_and_tokenizer(model_name, bnb_cfg):\n",
    "    print(f'Attempting to load: {model_name}')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "final_model_id = MODEL_ID\n",
    "tokenizer = None\n",
    "model = None\n",
    "try:\n",
    "    tokenizer, model = try_load_model_and_tokenizer(MODEL_ID, bnb_config)\n",
    "except Exception as e:\n",
    "    print('Primary load failed:', e)\n",
    "    err = str(e).lower()\n",
    "    if ('gated' in err) or ('403' in err) or ('access to model' in err) or ('you are trying to access a gated repo' in err):\n",
    "        print('Detected gated-access issue â€” falling back to', FALLBACK_MODEL)\n",
    "        final_model_id = FALLBACK_MODEL\n",
    "        tokenizer, model = try_load_model_and_tokenizer(FALLBACK_MODEL, bnb_config)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print('Using model:', final_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'Loading dataset {DATASET} split={DATASET_SPLIT} ...')\n",
    "ds = load_dataset(DATASET, split=DATASET_SPLIT)\n",
    "print('Dataset columns:', ds.column_names)\n",
    "print('Total rows:', len(ds))\n",
    "if SMOKE_TEST:\n",
    "    ds = ds.shuffle(seed=42).select(range(min(SMOKE_TEST_SIZE, len(ds))))\n",
    "    print('Smoke test subset size:', len(ds))\n",
    "print('Example row:', ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer is None:\n",
    "    raise RuntimeError('Tokenizer not loaded. Run the model-loading cell first.')\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def encode_messages(messages, max_length=MAX_SEQ_LENGTH):\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors='pt')\n",
    "    txt = '\\n\\n'.join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "    return tokenizer(txt, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "\n",
    "print('Tokenizer helper ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Preparing model for k-bit training ...')\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "def auto_find_target_modules(model, substrings=None, top_k=20):\n",
    "    if substrings is None:\n",
    "        substrings = ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj','wq','wk','wv','wo','fc','lin','down','up']\n",
    "    found = set()\n",
    "    for name, module in model.named_modules():\n",
    "        for s in substrings:\n",
    "            if s in name:\n",
    "                found.add(name.split('.')[-1])\n",
    "    if not found:\n",
    "        return ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj']\n",
    "    return list(found)[:top_k]\n",
    "\n",
    "lora_target_modules = auto_find_target_modules(model)\n",
    "print('Auto-detected target modules:', lora_target_modules)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "print('Applying LoRA...')\n",
    "model = get_peft_model(model, lora_config)\n",
    "try:\n",
    "    model.print_trainable_parameters()\n",
    "except Exception:\n",
    "    print('Trainable parameter summary not available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'Loading dataset {DATASET} split={DATASET_SPLIT} ...')\n",
    "ds_full = load_dataset(DATASET, split=DATASET_SPLIT)\n",
    "\n",
    "subset_size = 10000 \n",
    "ds = ds_full.shuffle(seed=42).select(range(subset_size))\n",
    "print(f\"Using a smaller subset of {len(ds)} rows for training.\")\n",
    "\n",
    "\n",
    "print('Dataset columns:', ds.column_names)\n",
    "print('Total rows:', len(ds))\n",
    "\n",
    "print('Example row:', ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print('Preparing SFTTrainer with MINIMAL legacy arguments...')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print('Trainer ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "correct_path = None\n",
    "\n",
    "for root, dirs, files in os.walk('/kaggle/input/'):\n",
    "    \n",
    "    if 'adapter_config.json' in files:\n",
    "        \n",
    "        correct_path = root\n",
    "        break\n",
    "\n",
    "if correct_path:\n",
    "    print(\"âœ… Found the correct adapter path!\")\n",
    "    print(f\"Correct Path is: {correct_path}\")\n",
    "else:\n",
    "    print(\"âŒ Could not find 'adapter_config.json' in /kaggle/input/. Please double-check your input files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T18:42:41.833826Z",
     "iopub.status.busy": "2025-09-15T18:42:41.833151Z",
     "iopub.status.idle": "2025-09-15T18:43:05.422075Z",
     "shell.execute_reply": "2025-09-15T18:43:05.421172Z",
     "shell.execute_reply.started": "2025-09-15T18:42:41.833794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Logging into Hugging Face...\n",
      "â— Kaggle secret 'HF_TOKEN' not found.\n",
      "ğŸš€ Loading the specialized SQL model: defog/sqlcoder-7b-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ef5868f93e4cb1aeb2e32d2e4df6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SQL Agent is ready.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "MODEL_ID = \"defog/sqlcoder-7b-2\"\n",
    "\n",
    "print(\"Logging into Hugging Face...\")\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Login successful.\")\n",
    "else:\n",
    "    print(\" Kaggle secret 'HF_TOKEN' not found.\")\n",
    "\n",
    "\n",
    "print(f\"Loading the specialized SQL model: {MODEL_ID}...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "agent_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "agent_model.eval()\n",
    "print(\" SQL Agent is ready.\")\n",
    "\n",
    "\n",
    "def generate_sql(natural_language_query, db_schema):\n",
    " \n",
    "    prompt = f\"\"\"### Task\n",
    "Generate a SQL query to answer the following question based on the database schema.\n",
    "\n",
    "### Database Schema\n",
    "{db_schema}\n",
    "\n",
    "### Question\n",
    "`{natural_language_query}`\n",
    "\n",
    "### SQL Query\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(agent_model.device)\n",
    "\n",
    "    outputs = agent_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    sql_query = response.split(\"### SQL Query\")[-1].strip()\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:13:32.971693Z",
     "iopub.status.busy": "2025-09-17T13:13:32.971409Z",
     "iopub.status.idle": "2025-09-17T13:13:32.975758Z",
     "shell.execute_reply": "2025-09-17T13:13:32.975001Z",
     "shell.execute_reply.started": "2025-09-17T13:13:32.971672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db_schema = \"\"\"\n",
    "CREATE TABLE Departments (\n",
    "    COUNTRY VARCHAR(255),\n",
    "    LAST_NAME VARCHAR(255),\n",
    "    FIRST_NAME VARCHAR(255),\n",
    "    SALARY VARCHAR(255),\n",
    "    CITY VARCHAR(255),\n",
    "    DEPARTMENT VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Student (\n",
    "    SALARY VARCHAR(255),\n",
    "    LAST_NAME VARCHAR(255),\n",
    "    FIRST_NAME VARCHAR(255),\n",
    "    DEPARTMENT VARCHAR(255),\n",
    "    CITY VARCHAR(255),\n",
    "    COUNTRY VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Orders (\n",
    "    LAST_NAME VARCHAR(255),\n",
    "    FIRST_NAME VARCHAR(255),\n",
    "    DEPARTMENT VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Employee (\n",
    "    LAST_NAME VARCHAR(255),\n",
    "    SALARY VARCHAR(255),\n",
    "    CITY VARCHAR(255),\n",
    "    FIRST_NAME VARCHAR(255),\n",
    "    DEPARTMENT VARCHAR(255),\n",
    "    COUNTRY VARCHAR(255)\n",
    ");\n",
    "\n",
    "CREATE TABLE Products (\n",
    "    SALARY VARCHAR(255),\n",
    "    LAST_NAME VARCHAR(255),\n",
    "    FIRST_NAME VARCHAR(255),\n",
    "    COUNTRY VARCHAR(255),\n",
    "    CITY VARCHAR(255),\n",
    "    DEPARTMENT VARCHAR(255)\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:28:09.560086Z",
     "iopub.status.busy": "2025-09-17T13:28:09.559382Z",
     "iopub.status.idle": "2025-09-17T13:28:46.583154Z",
     "shell.execute_reply": "2025-09-17T13:28:46.582478Z",
     "shell.execute_reply.started": "2025-09-17T13:28:09.560058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Logging into Hugging Face...\n",
      "â— Kaggle secret 'HF_TOKEN' not found.\n",
      "ğŸš€ Loading the specialized SQL model: defog/sqlcoder-7b-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc81154d3c24aa8baa5e3f54b12b27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SQL Agent is ready.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "MODEL_ID = \"defog/sqlcoder-7b-2\"\n",
    "\n",
    "print(\"ğŸš€ Logging into Hugging Face...\")\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"âœ… Login successful.\")\n",
    "else:\n",
    "    print(\"â— Kaggle secret 'HF_TOKEN' not found.\")\n",
    "\n",
    "print(f\"ğŸš€ Loading the specialized SQL model: {MODEL_ID}...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "agent_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "agent_model.eval()\n",
    "print(\"âœ… SQL Agent is ready.\")\n",
    "\n",
    "\n",
    "def generate_sql(natural_language_query, schema):\n",
    "    \"\"\"\n",
    "    Creates a highly explicit, schema-aware prompt for sqlcoder using a few-shot example to improve accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"### Task\n",
    "You are an expert SQL writer. Your task is to generate a single, syntactically correct SQL query to answer the user's question, based on the provided database schema.\n",
    "Pay close attention to the table and column names and ensure the query logic correctly answers the question.\n",
    "\n",
    "---\n",
    "### Example\n",
    "\n",
    "### Database Schema\n",
    "CREATE TABLE Products (PRODUCT_ID INT, PRODUCT_NAME VARCHAR(50), PRICE DECIMAL(10, 2));\n",
    "\n",
    "### Question\n",
    "`List all products that cost less than 50 dollars`\n",
    "\n",
    "### SQL Query\n",
    "SELECT PRODUCT_NAME FROM Products WHERE PRICE < 50;\n",
    "---\n",
    "\n",
    "### Database Schema\n",
    "{schema}\n",
    "\n",
    "### Question\n",
    "`{natural_language_query}`\n",
    "\n",
    "### SQL Query\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(agent_model.device)\n",
    "    \n",
    "    outputs = agent_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sql_query = response.split(\"### SQL Query\")[-1].strip()\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:14:33.062063Z",
     "iopub.status.busy": "2025-09-17T13:14:33.061527Z",
     "iopub.status.idle": "2025-09-17T13:19:41.830639Z",
     "shell.execute_reply": "2025-09-17T13:19:41.829775Z",
     "shell.execute_reply.started": "2025-09-17T13:14:33.062038Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In-memory test database with correct schema is ready.\n",
      "Loading validation dataset...\n",
      "Starting evaluation on 100 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:06<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Evaluation Results ---\n",
      "Execution Accuracy (runs without error): 83.00%\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "for statement in db_schema.strip().split(';'):\n",
    "    if statement.strip():\n",
    "        cursor.execute(statement)\n",
    "print(\"âœ… In-memory test database with correct schema is ready.\")\n",
    "\n",
    "\n",
    "print(\"Loading validation dataset...\")\n",
    "full_ds = load_dataset(\"Soft2012/sql_fine_tune_dataset\", split=\"train\")\n",
    "test_ds = full_ds.select(range(20000, 20100)) \n",
    "\n",
    "def parse_example(example):\n",
    "    text = example['text']\n",
    "    parts = text.split(\"[/INST]\")\n",
    "    question = parts[0].split(\"Write a SQL query to\")[-1].strip()\n",
    "    return question\n",
    "\n",
    "\n",
    "execution_successes = 0\n",
    "total = len(test_ds)\n",
    "print(f\"Starting evaluation on {total} examples...\")\n",
    "\n",
    "for example in tqdm(test_ds):\n",
    "    question = parse_example(example)\n",
    "    generated_sql = generate_sql(question, db_schema)\n",
    "        \n",
    "\n",
    "    try:\n",
    "        cursor.execute(generated_sql)\n",
    "        execution_successes += 1\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "execution_accuracy = (execution_successes / total) * 100\n",
    "\n",
    "print(f\"\\n--- Final Evaluation Results ---\")\n",
    "print(f\"Execution Accuracy (runs without error): {execution_accuracy:.2f}%\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:52:21.110193Z",
     "iopub.status.busy": "2025-09-17T13:52:21.109562Z",
     "iopub.status.idle": "2025-09-17T13:52:24.306646Z",
     "shell.execute_reply": "2025-09-17T13:52:24.305671Z",
     "shell.execute_reply.started": "2025-09-17T13:52:21.110166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Configuring Gemini for Planner and Joiner agents...\n",
      "âœ… Gemini configured successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "print(\"ğŸš€ Configuring Gemini for Planner and Joiner agents...\")\n",
    "\n",
    "gemini_api_key = \"GEMINI KEY\"\n",
    "if gemini_api_key:\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "    planner_model = genai.GenerativeModel('gemini-1.5-flash', generation_config={\"response_mime_type\": \"application/json\"})\n",
    "    joiner_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    print(\"âœ… Gemini configured successfully.\")\n",
    "else:\n",
    "    print(\"â— Kaggle secret 'GOOGLE_API_KEY' not found. Planner/Joiner will not work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:25:51.654004Z",
     "iopub.status.busy": "2025-09-17T14:25:51.653705Z",
     "iopub.status.idle": "2025-09-17T14:25:51.659725Z",
     "shell.execute_reply": "2025-09-17T14:25:51.658877Z",
     "shell.execute_reply.started": "2025-09-17T14:25:51.653982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Planner agent defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def planner_agent(user_query, schema):\n",
    "    \"\"\"Takes a user query and returns a structured plan using Gemini.\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a master planner agent. Your goal is to create a step-by-step plan to answer the user's request.\n",
    "\n",
    "Each step must be a JSON object with this format:\n",
    "{{\n",
    "  \"step_number\": <int>,\n",
    "  \"thought\": \"<your reasoning>\",\n",
    "  \"action\": {{\n",
    "      \"type\": \"<respond_to_user | call_tool | join_results>\",\n",
    "      \"details\": \"<text or natural language sub-question>\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Database Schema:\n",
    "{schema}\n",
    "\n",
    "User Request:\n",
    "`{user_query}`\n",
    "\n",
    "Return ONLY valid JSON with a single key \"plan\".\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = planner_model.generate_content(prompt)\n",
    "        plan = json.loads(response.text)\n",
    "        return plan.get(\"plan\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini Planner API: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"âœ… Planner agent defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:59:37.983690Z",
     "iopub.status.busy": "2025-09-17T13:59:37.983354Z",
     "iopub.status.idle": "2025-09-17T13:59:37.989779Z",
     "shell.execute_reply": "2025-09-17T13:59:37.989098Z",
     "shell.execute_reply.started": "2025-09-17T13:59:37.983665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Joiner agent defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def joiner_agent(user_query, evidence):\n",
    "    \"\"\"\n",
    "    Synthesizes a final answer using Gemini from structured evidence.\n",
    "    Evidence should be a list of dicts with keys 'sql' and 'data'.\n",
    "    \"\"\"\n",
    "\n",
    "    evidence_str = \"\\n\".join([f\"- Step {i+1}: {item}\" for i, item in enumerate(evidence)])\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. \n",
    "Your job is to answer the user's original question by summarizing the evidence provided from tool calls.  \n",
    "\n",
    "Synthesize the information from the 'Evidence' section into a single, clear, and friendly answer.  \n",
    "When the evidence contains a list of items, you MUST list all of the items in your final answer.\n",
    "\n",
    "---\n",
    "### Original Question:\n",
    "{user_query}\n",
    "\n",
    "### Evidence:\n",
    "{evidence_str}\n",
    "\n",
    "### Final Answer:\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = joiner_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini Joiner API: {e}\")\n",
    "        return \"Sorry, I couldn't synthesize the results.\"\n",
    "\n",
    "print(\"âœ… Joiner agent defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:43:47.035705Z",
     "iopub.status.busy": "2025-09-17T14:43:47.035382Z",
     "iopub.status.idle": "2025-09-17T14:43:47.044631Z",
     "shell.execute_reply": "2025-09-17T14:43:47.043748Z",
     "shell.execute_reply.started": "2025-09-17T14:43:47.035680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def execute_sql(sql_query):\n",
    "    \"\"\"\n",
    "    Executes a SQL query against the in-memory database\n",
    "    and returns the result.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(sql_query)\n",
    "        result = cursor.fetchall()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "       \n",
    "        return f\"Error executing query: {e}\"\n",
    "\n",
    "\n",
    "def orchestrator(user_query, schema):\n",
    "    print(f\"\\nğŸ¬ User Query: '{user_query}'\\n\")\n",
    "\n",
    "\n",
    "    plan = planner_agent(user_query, schema)\n",
    "    print(\"ğŸ“ Planner's Plan:\")\n",
    "    for step in plan:\n",
    "        print(f\"  - Step {step['step_number']}: {step['thought']}\")\n",
    "    print(\"\\nğŸš€ Executing Plan...\")\n",
    "\n",
    "    query_result = None  \n",
    "\n",
    "    for step in plan:\n",
    "        step_num = step[\"step_number\"]\n",
    "        action = step[\"action\"][\"type\"]\n",
    "        details = step[\"action\"][\"details\"]\n",
    "\n",
    "        print(f\"\\n>> Executing Step {step_num} ({action})...\")\n",
    "\n",
    "        if action == \"call_tool\":\n",
    "            sql = generate_sql(details, schema)\n",
    "            print(f\"  - Generated SQL: {sql}\")\n",
    "            query_result = execute_sql(sql)  \n",
    "            print(f\"  - Query Result: {query_result}\")\n",
    "\n",
    "        elif action in [\"respond_to_user\", \"join_results\"]:\n",
    "            \n",
    "            if query_result and len(query_result) > 0:\n",
    "                if len(query_result[0]) == 1:\n",
    "                    \n",
    "                    answer = query_result[0][0]\n",
    "                else:\n",
    "                    answer = \" \".join(str(x) for x in query_result[0])\n",
    "                print(f\"  - Direct Response: The answer is {answer}.\")\n",
    "            else:\n",
    "                print(\"  - Direct Response: Sorry, no data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T13:59:44.365367Z",
     "iopub.status.busy": "2025-09-17T13:59:44.364755Z",
     "iopub.status.idle": "2025-09-17T14:01:40.236007Z",
     "shell.execute_reply": "2025-09-17T14:01:40.235393Z",
     "shell.execute_reply.started": "2025-09-17T13:59:44.365338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ User Query: 'Find the name of the employee with the highest salary, and then list all products that cost less than half of that salary.'\n",
      "\n",
      "ğŸ“ Planner's Plan:\n",
      "  - Step 1: First, I need to find the highest employee salary.\n",
      "  - Step 2: Now that I have the highest salary, I need to find all products costing less than half of that salary.\n",
      "  - Step 3: The results from the previous steps need to be combined to answer the user's request. The first step gives the highest salary, which will be needed for the second step, and the second step gives a list of products that meet the criteria.\n",
      "\n",
      "ğŸš€ Executing Plan...\n",
      "\n",
      ">> Executing Step 1 (call_tool)...\n",
      "  - Generated SQL: SELECT MAX(SALARY) AS highest_salary FROM Employee;\n",
      "  - Query Result: [(150000,)]\n",
      "\n",
      ">> Executing Step 2 (call_tool)...\n",
      "  - Generated SQL: SELECT PRODUCT_NAME FROM Products WHERE PRICE < (SELECT MAX(SALARY) / 2 FROM Employee);\n",
      "  - Query Result: [('Laptop',), ('Mouse',), ('Keyboard',), ('Monitor',)]\n",
      "\n",
      ">> Executing Step 3 (join_results)...\n",
      "  - Final Answer: The employee with the highest salary makes $150,000.  Products costing less than half of that ($75,000) include: Laptop, Mouse, Keyboard, and Monitor.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The employee with the highest salary makes $150,000.  Products costing less than half of that ($75,000) include: Laptop, Mouse, Keyboard, and Monitor.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "complex_query = \"Find the name of the employee with the highest salary, and then list all products that cost less than half of that salary.\"\n",
    "\n",
    "orchestrator(complex_query, db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:43:51.130960Z",
     "iopub.status.busy": "2025-09-17T14:43:51.130353Z",
     "iopub.status.idle": "2025-09-17T14:45:35.446113Z",
     "shell.execute_reply": "2025-09-17T14:45:35.445262Z",
     "shell.execute_reply.started": "2025-09-17T14:43:51.130936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Multi-Agent SQL Assistant is now running.\n",
      "   Enter your complex request, or type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Who is the employee earning the highest salary, and in which city do they live?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ User Query: 'Who is the employee earning the highest salary, and in which city do they live?'\n",
      "\n",
      "ğŸ“ Planner's Plan:\n",
      "  - Step 1: I need to find the employee with the highest salary.  The database schema doesn't include city information, so I cannot answer the second part of the question.\n",
      "  - Step 2: To find the highest salary, I need to query the `Employee` table and order by salary in descending order, limiting the results to one row.\n",
      "  - Step 3: The tool will return the first name and last name of the highest-earning employee. I will format this into a user-friendly response.\n",
      "\n",
      "ğŸš€ Executing Plan...\n",
      "\n",
      ">> Executing Step 1 (respond_to_user)...\n",
      "  - Direct Response: Sorry, no data found.\n",
      "\n",
      ">> Executing Step 2 (call_tool)...\n",
      "  - Generated SQL: SELECT FIRST_NAME, LAST_NAME FROM Employee ORDER BY SALARY DESC LIMIT 1;\n",
      "  - Query Result: [('John', 'Doe')]\n",
      "\n",
      ">> Executing Step 3 (join_results)...\n",
      "  - Direct Response: The answer is John Doe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Assistant shutting down. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ¤– Multi-Agent SQL Assistant is now running.\")\n",
    "print(\"   Enter your complex request, or type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in ['exit', 'quit']:\n",
    "        print(\"ğŸ¤– Assistant shutting down. Goodbye!\")\n",
    "        break\n",
    "    orchestrator(user_query, db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 450595,
     "modelInstanceId": 433741,
     "sourceId": 581100,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
